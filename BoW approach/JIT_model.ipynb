{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e6db105-253c-4b41-b2e2-97fe008f31c7",
   "metadata": {},
   "source": [
    "### Finding the main method of a java file and adding @Benchmark annotation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c793dd34-b46c-4836-b489-b6f79715ed0b",
   "metadata": {},
   "source": [
    "This program will iterate over every file in the directory and add @Benchmark tag over each function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "204ff9aa-0cfb-44cc-8e86-20d35c3caa81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed annotation for file : ForwardBackward.java \n",
      "Completed annotation for file : Recursive.java \n",
      "Completed annotation for file : Shifting.java \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "# Define the folder containing Java files\n",
    "folder_path = r'C:\\Users\\Mettle\\Desktop\\Java-master\\java-combinations\\src\\main\\java\\com\\hmkcode'\n",
    "\n",
    "# Create the 'benchmark_files' folder if it doesn't exist\n",
    "if not os.path.exists('benchmark_files'):\n",
    "    os.makedirs('benchmark_files')\n",
    "\n",
    "# Function to process each Java file\n",
    "def process_java_file(java_file_path):\n",
    "    benchmark_file_path = os.path.join('benchmark_files', os.path.basename(java_file_path))\n",
    "    add_benchmark_annotation(java_file_path, benchmark_file_path)\n",
    "    add_counters(benchmark_file_path)\n",
    "    method_invocation_annotation(benchmark_file_path)\n",
    "   \n",
    "# Iterate over each file in the folder\n",
    "for file_name in os.listdir(folder_path):\n",
    "    if file_name.endswith('.java'):\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "        process_java_file(file_path)\n",
    "        print(f\"Completed annotation for file : {file_name} \")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cab5b9a-2d4b-49f9-b3ae-0c050d45e39c",
   "metadata": {},
   "source": [
    "#### Loop counter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c553d0ad-1976-4a90-be21-bb00cf18b5b0",
   "metadata": {},
   "source": [
    "*Currently not handling case with loops without braces.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d1fdf559-bb59-4e5a-8244-713c196ec384",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_counters(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "            java_code = file.read()\n",
    "    # Regex pattern for finding for loops and while loops\n",
    "    pattern = r'(?P<loop>(for\\s*\\([^\\)]*\\)\\s*\\{)|(while\\s*\\([^\\)]*\\)\\s*\\{))'\n",
    "    \n",
    "    def add_counter(match):\n",
    "        loop = match.group('loop')\n",
    "        return loop + '\\n    counter++;'\n",
    "    \n",
    "    # Replace each loop with the loop plus counter++\n",
    "    modified_java_code = re.sub(pattern, add_counter, java_code)\n",
    "    with open(file_path, 'w') as file:\n",
    "        file.write(modified_java_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8afbe892-3588-4713-b559-34fc72f244aa",
   "metadata": {},
   "source": [
    "#### @Benchmark Annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9a0d1066-9a92-4f1c-a1e2-03b2382451fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_benchmark_annotation(file_path, benchmark_file_path):\n",
    "    pattern = r'(?<!\\/\\/)(?<!\\/\\*)\\b(?:public\\s+|private\\s+|protected\\s+|static\\s+|final\\s+|native\\s+|synchronized\\s+|abstract\\s+|transient\\s+)*[\\$_\\w<>\\[\\]]*\\s+\\w+\\s*\\([^\\)]*\\)?\\s*\\{'\n",
    "\n",
    "    with open(file_path, 'r') as file:\n",
    "        java_code = file.read()\n",
    "\n",
    "    # Find all occurrences of method declarations in the Java code\n",
    "    matches = re.finditer(pattern, java_code)\n",
    "\n",
    "    modified_java_code = ''\n",
    "    previous_end_index = 0\n",
    "\n",
    "    # Loop through each method occurrence\n",
    "    for match in matches:\n",
    "        start_index = match.start()\n",
    "        end_index = match.end()\n",
    "        \n",
    "        method_declaration = java_code[start_index:end_index]\n",
    "        \n",
    "        # Insert @Benchmark annotation just above the method declaration\n",
    "        modified_java_code += java_code[previous_end_index:start_index] + '@Benchmark\\n' + method_declaration\n",
    "\n",
    "        previous_end_index = end_index\n",
    "\n",
    "    modified_java_code += java_code[previous_end_index:]\n",
    "    with open(benchmark_file_path, 'w') as file:\n",
    "        file.write(modified_java_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "302ebcf6-df6c-493e-90a3-55709e87819b",
   "metadata": {},
   "source": [
    "#### Count method invocation values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "73787349-7e31-4974-94c2-00320c8eae95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def method_invocation_annotation(file_path):\n",
    "    method_pattern = r'(?P<method>(?<!\\/\\/)(?<!\\/\\*)\\b(?:public\\s+|private\\s+|protected\\s+|static\\s+|final\\s+|native\\s+|synchronized\\s+|abstract\\s+|transient\\s+)*[\\$_\\w<>\\[\\]]*\\s+\\w+\\s*\\([^\\)]*\\)\\s*\\{[^\\}]*?\\})'\n",
    "    with open(file_path, 'r') as file:\n",
    "        java_code = file.read()\n",
    "    def add_counter(match):\n",
    "        method = match.group()\n",
    "        modified_method = method.replace('{', '{\\n    method_counter++;\\n')\n",
    "        return modified_method\n",
    "\n",
    "    # Replace each method with additional code added inside\n",
    "    modified_java_code = re.sub(method_pattern, add_counter, java_code)\n",
    "    with open(file_path, 'w') as file:\n",
    "        file.write(modified_java_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e131558-8d65-418e-aa71-8814bcd2ecbe",
   "metadata": {},
   "source": [
    "# Getting Codeforces submissions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7c79329-86bd-432a-8de9-60144297e090",
   "metadata": {},
   "source": [
    "### Generate URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d0bfd351-d379-4df9-a216-4e0d84b930c8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import hashlib\n",
    "import time\n",
    "import random\n",
    "from bs4 import BeautifulSoup\n",
    "def get_url(from_val):\n",
    "    key = \"baa5c566fa5bdeb92494876e5bcac06b6798d8fe\"\n",
    "    secret = \"0d686bfc8a3c654a569b93e3d1d12803d25264f1\"\n",
    "    count = 1000\n",
    "    current_time = int(time.time())\n",
    "    random_number = random.randint(100000, 999999)\n",
    "    method_fetched = \"contest.status\"\n",
    "    string_to_hash = f\"{random_number}/{method_fetched}?apiKey={key}&contestId=1928&count={count}&from={from_val}&time={current_time}#{secret}\"\n",
    "    sha512_hash = hashlib.sha512(string_to_hash.encode()).hexdigest()\n",
    "    req_url = f\"https://codeforces.com/api/{method_fetched}?apiKey={key}&contestId=1928&count={count}&from={from_val}&time={current_time}&apiSig={random_number}{sha512_hash}\"\n",
    "    return req_url"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd8cd6ee-50a2-4773-b9de-9bd2ab0afdb8",
   "metadata": {},
   "source": [
    "### Next step is to make a crawler to incrementally fetch data from Codeforces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8ce05d3b-bb1d-4f12-8a55-94bb39875841",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import os.path\n",
    "from os import path\n",
    "\n",
    "# Function to check if file exists and create it if it doesn't\n",
    "def create_csv_file(file_path):\n",
    "    if not path.exists(file_path):\n",
    "        with open(file_path, 'w', newline='') as file:\n",
    "            # Create a CSV writer object\n",
    "            csv_writer = csv.writer(file)\n",
    "            # Write header row\n",
    "            csv_writer.writerow(['id', 'contest_id', 'author','creation_time_seconds', 'relative_time_seconds', 'problem_name', 'problem_type', 'programming_language', 'verdict', 'test_set', 'passed_test_count', 'time_consumed_millis', 'memory_consumed_bytes'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d7409cde-548f-4055-b8a5-55c483b2330d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_to_csv(data, file_path):\n",
    "    fieldnames = ['id', 'contest_id', 'author','creation_time_seconds', 'relative_time_seconds', 'problem_name', 'problem_type', 'programming_language', 'verdict', 'test_set', 'passed_test_count', 'time_consumed_millis', 'memory_consumed_bytes']\n",
    "    \n",
    "    file_exists = os.path.isfile(file_path)\n",
    "    \n",
    "    with open(file_path, 'a', newline='') as file:\n",
    "        writer = csv.DictWriter(file, fieldnames=fieldnames)\n",
    "        if not file_exists:\n",
    "            writer.writeheader()  # Write header only if file is new\n",
    "        writer.writerow(data)\n",
    "\n",
    "def make_request_and_append_to_csv(from_val, file_path):\n",
    "    req_url = get_url(from_val)\n",
    "    response = requests.get(req_url)\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        for item in data['result']:\n",
    "            csv_data = {\n",
    "                'id': item['id'],\n",
    "                'contest_id': item['contestId'],\n",
    "                'author': item['author']['members'][0]['handle'],\n",
    "                'creation_time_seconds': item['creationTimeSeconds'],\n",
    "                'relative_time_seconds': item['relativeTimeSeconds'],\n",
    "                'problem_name': item['problem']['name'],\n",
    "                'problem_type': item['problem']['type'],\n",
    "                'programming_language': item['programmingLanguage'],\n",
    "                'verdict': item['verdict'],\n",
    "                'test_set': item['testset'],\n",
    "                'passed_test_count': item['passedTestCount'],\n",
    "                'time_consumed_millis': item['timeConsumedMillis'],\n",
    "                'memory_consumed_bytes': item['memoryConsumedBytes']\n",
    "            }\n",
    "            append_to_csv(csv_data, file_path)\n",
    "    else:\n",
    "        print(\"Request failed with status code:\", response.status_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2232d031-4501-456e-bdb5-e0dd4dc97aa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed: 0 / 200 , Index value : 1001\n",
      "Completed: 1 / 200 , Index value : 2001\n",
      "Completed: 2 / 200 , Index value : 3001\n",
      "Completed: 3 / 200 , Index value : 4001\n",
      "Completed: 4 / 200 , Index value : 5001\n",
      "Completed: 5 / 200 , Index value : 6001\n",
      "Completed: 6 / 200 , Index value : 7001\n",
      "Completed: 7 / 200 , Index value : 8001\n",
      "Completed: 8 / 200 , Index value : 9001\n",
      "Completed: 9 / 200 , Index value : 10001\n",
      "Completed: 10 / 200 , Index value : 11001\n",
      "Completed: 11 / 200 , Index value : 12001\n",
      "Completed: 12 / 200 , Index value : 13001\n",
      "Completed: 13 / 200 , Index value : 14001\n",
      "Completed: 14 / 200 , Index value : 15001\n",
      "Completed: 15 / 200 , Index value : 16001\n",
      "Completed: 16 / 200 , Index value : 17001\n",
      "Completed: 17 / 200 , Index value : 18001\n",
      "Completed: 18 / 200 , Index value : 19001\n",
      "Completed: 19 / 200 , Index value : 20001\n",
      "Completed: 20 / 200 , Index value : 21001\n",
      "Completed: 21 / 200 , Index value : 22001\n",
      "Completed: 22 / 200 , Index value : 23001\n",
      "Completed: 23 / 200 , Index value : 24001\n",
      "Completed: 24 / 200 , Index value : 25001\n",
      "Completed: 25 / 200 , Index value : 26001\n",
      "Completed: 26 / 200 , Index value : 27001\n",
      "Completed: 27 / 200 , Index value : 28001\n",
      "Completed: 28 / 200 , Index value : 29001\n",
      "Completed: 29 / 200 , Index value : 30001\n",
      "Completed: 30 / 200 , Index value : 31001\n",
      "Completed: 31 / 200 , Index value : 32001\n",
      "Completed: 32 / 200 , Index value : 33001\n",
      "Completed: 33 / 200 , Index value : 34001\n",
      "Completed: 34 / 200 , Index value : 35001\n",
      "Completed: 35 / 200 , Index value : 36001\n",
      "Completed: 36 / 200 , Index value : 37001\n",
      "Completed: 37 / 200 , Index value : 38001\n",
      "Completed: 38 / 200 , Index value : 39001\n",
      "Completed: 39 / 200 , Index value : 40001\n",
      "Completed: 40 / 200 , Index value : 41001\n",
      "Completed: 41 / 200 , Index value : 42001\n",
      "Completed: 42 / 200 , Index value : 43001\n",
      "Completed: 43 / 200 , Index value : 44001\n",
      "Completed: 44 / 200 , Index value : 45001\n",
      "Completed: 45 / 200 , Index value : 46001\n",
      "Completed: 46 / 200 , Index value : 47001\n",
      "Completed: 47 / 200 , Index value : 48001\n",
      "Completed: 48 / 200 , Index value : 49001\n",
      "Completed: 49 / 200 , Index value : 50001\n",
      "Completed: 50 / 200 , Index value : 51001\n",
      "Completed: 51 / 200 , Index value : 52001\n",
      "Completed: 52 / 200 , Index value : 53001\n",
      "Completed: 53 / 200 , Index value : 54001\n",
      "Completed: 54 / 200 , Index value : 55001\n",
      "Completed: 55 / 200 , Index value : 56001\n",
      "Completed: 56 / 200 , Index value : 57001\n",
      "Completed: 57 / 200 , Index value : 58001\n",
      "Completed: 58 / 200 , Index value : 59001\n",
      "Completed: 59 / 200 , Index value : 60001\n",
      "Completed: 60 / 200 , Index value : 61001\n",
      "Completed: 61 / 200 , Index value : 62001\n",
      "Completed: 62 / 200 , Index value : 63001\n",
      "Completed: 63 / 200 , Index value : 64001\n",
      "Completed: 64 / 200 , Index value : 65001\n",
      "Completed: 65 / 200 , Index value : 66001\n",
      "Completed: 66 / 200 , Index value : 67001\n",
      "Completed: 67 / 200 , Index value : 68001\n",
      "Completed: 68 / 200 , Index value : 69001\n",
      "Completed: 69 / 200 , Index value : 70001\n",
      "Completed: 70 / 200 , Index value : 71001\n",
      "Completed: 71 / 200 , Index value : 72001\n",
      "Completed: 72 / 200 , Index value : 73001\n",
      "Completed: 73 / 200 , Index value : 74001\n",
      "Completed: 74 / 200 , Index value : 75001\n",
      "Completed: 75 / 200 , Index value : 76001\n",
      "Completed: 76 / 200 , Index value : 77001\n",
      "Completed: 77 / 200 , Index value : 78001\n",
      "Completed: 78 / 200 , Index value : 79001\n",
      "Completed: 79 / 200 , Index value : 80001\n",
      "Completed: 80 / 200 , Index value : 81001\n",
      "Completed: 81 / 200 , Index value : 82001\n",
      "Completed: 82 / 200 , Index value : 83001\n",
      "Completed: 83 / 200 , Index value : 84001\n",
      "Completed: 84 / 200 , Index value : 85001\n",
      "Completed: 85 / 200 , Index value : 86001\n",
      "Completed: 86 / 200 , Index value : 87001\n",
      "Completed: 87 / 200 , Index value : 88001\n",
      "Completed: 88 / 200 , Index value : 89001\n",
      "Completed: 89 / 200 , Index value : 90001\n",
      "Completed: 90 / 200 , Index value : 91001\n",
      "Completed: 91 / 200 , Index value : 92001\n",
      "Completed: 92 / 200 , Index value : 93001\n",
      "Completed: 93 / 200 , Index value : 94001\n",
      "Completed: 94 / 200 , Index value : 95001\n",
      "Completed: 95 / 200 , Index value : 96001\n",
      "Completed: 96 / 200 , Index value : 97001\n",
      "Completed: 97 / 200 , Index value : 98001\n",
      "Completed: 98 / 200 , Index value : 99001\n",
      "Completed: 99 / 200 , Index value : 100001\n",
      "Completed: 100 / 200 , Index value : 101001\n",
      "Completed: 101 / 200 , Index value : 102001\n",
      "Completed: 102 / 200 , Index value : 103001\n",
      "Completed: 103 / 200 , Index value : 104001\n",
      "Completed: 104 / 200 , Index value : 105001\n",
      "Completed: 105 / 200 , Index value : 106001\n",
      "Completed: 106 / 200 , Index value : 107001\n",
      "Completed: 107 / 200 , Index value : 108001\n",
      "Completed: 108 / 200 , Index value : 109001\n",
      "Completed: 109 / 200 , Index value : 110001\n",
      "Completed: 110 / 200 , Index value : 111001\n",
      "Completed: 111 / 200 , Index value : 112001\n",
      "Completed: 112 / 200 , Index value : 113001\n",
      "Completed: 113 / 200 , Index value : 114001\n",
      "Completed: 114 / 200 , Index value : 115001\n",
      "Completed: 115 / 200 , Index value : 116001\n",
      "Completed: 116 / 200 , Index value : 117001\n",
      "Completed: 117 / 200 , Index value : 118001\n",
      "Completed: 118 / 200 , Index value : 119001\n",
      "Completed: 119 / 200 , Index value : 120001\n",
      "Completed: 120 / 200 , Index value : 121001\n",
      "Completed: 121 / 200 , Index value : 122001\n",
      "Completed: 122 / 200 , Index value : 123001\n",
      "Completed: 123 / 200 , Index value : 124001\n",
      "Completed: 124 / 200 , Index value : 125001\n",
      "Completed: 125 / 200 , Index value : 126001\n",
      "Completed: 126 / 200 , Index value : 127001\n",
      "Completed: 127 / 200 , Index value : 128001\n",
      "Completed: 128 / 200 , Index value : 129001\n",
      "Completed: 129 / 200 , Index value : 130001\n",
      "Completed: 130 / 200 , Index value : 131001\n",
      "Completed: 131 / 200 , Index value : 132001\n",
      "Completed: 132 / 200 , Index value : 133001\n",
      "Completed: 133 / 200 , Index value : 134001\n",
      "Completed: 134 / 200 , Index value : 135001\n",
      "Completed: 135 / 200 , Index value : 136001\n",
      "Completed: 136 / 200 , Index value : 137001\n",
      "Completed: 137 / 200 , Index value : 138001\n",
      "Completed: 138 / 200 , Index value : 139001\n",
      "Completed: 139 / 200 , Index value : 140001\n",
      "Completed: 140 / 200 , Index value : 141001\n",
      "Completed: 141 / 200 , Index value : 142001\n",
      "Completed: 142 / 200 , Index value : 143001\n",
      "Completed: 143 / 200 , Index value : 144001\n",
      "Completed: 144 / 200 , Index value : 145001\n",
      "Completed: 145 / 200 , Index value : 146001\n",
      "Completed: 146 / 200 , Index value : 147001\n",
      "Completed: 147 / 200 , Index value : 148001\n",
      "Completed: 148 / 200 , Index value : 149001\n",
      "Completed: 149 / 200 , Index value : 150001\n",
      "Completed: 150 / 200 , Index value : 151001\n",
      "Completed: 151 / 200 , Index value : 152001\n",
      "Completed: 152 / 200 , Index value : 153001\n",
      "Completed: 153 / 200 , Index value : 154001\n",
      "Completed: 154 / 200 , Index value : 155001\n",
      "Completed: 155 / 200 , Index value : 156001\n",
      "Completed: 156 / 200 , Index value : 157001\n",
      "Completed: 157 / 200 , Index value : 158001\n",
      "Completed: 158 / 200 , Index value : 159001\n",
      "Completed: 159 / 200 , Index value : 160001\n",
      "Completed: 160 / 200 , Index value : 161001\n",
      "Completed: 161 / 200 , Index value : 162001\n",
      "Completed: 162 / 200 , Index value : 163001\n",
      "Completed: 163 / 200 , Index value : 164001\n",
      "Completed: 164 / 200 , Index value : 165001\n",
      "Completed: 165 / 200 , Index value : 166001\n",
      "Completed: 166 / 200 , Index value : 167001\n",
      "Completed: 167 / 200 , Index value : 168001\n",
      "Completed: 168 / 200 , Index value : 169001\n",
      "Completed: 169 / 200 , Index value : 170001\n",
      "Completed: 170 / 200 , Index value : 171001\n",
      "Completed: 171 / 200 , Index value : 172001\n",
      "Completed: 172 / 200 , Index value : 173001\n",
      "Completed: 173 / 200 , Index value : 174001\n",
      "Completed: 174 / 200 , Index value : 175001\n",
      "Completed: 175 / 200 , Index value : 176001\n",
      "Completed: 176 / 200 , Index value : 177001\n",
      "Completed: 177 / 200 , Index value : 178001\n",
      "Completed: 178 / 200 , Index value : 179001\n",
      "Completed: 179 / 200 , Index value : 180001\n",
      "Completed: 180 / 200 , Index value : 181001\n",
      "Completed: 181 / 200 , Index value : 182001\n",
      "Completed: 182 / 200 , Index value : 183001\n",
      "Completed: 183 / 200 , Index value : 184001\n",
      "Completed: 184 / 200 , Index value : 185001\n",
      "Completed: 185 / 200 , Index value : 186001\n",
      "Completed: 186 / 200 , Index value : 187001\n",
      "Completed: 187 / 200 , Index value : 188001\n",
      "Completed: 188 / 200 , Index value : 189001\n",
      "Completed: 189 / 200 , Index value : 190001\n",
      "Completed: 190 / 200 , Index value : 191001\n",
      "Completed: 191 / 200 , Index value : 192001\n",
      "Completed: 192 / 200 , Index value : 193001\n",
      "Completed: 193 / 200 , Index value : 194001\n",
      "Completed: 194 / 200 , Index value : 195001\n",
      "Completed: 195 / 200 , Index value : 196001\n",
      "Completed: 196 / 200 , Index value : 197001\n",
      "Completed: 197 / 200 , Index value : 198001\n",
      "Completed: 198 / 200 , Index value : 199001\n",
      "Completed: 199 / 200 , Index value : 200001\n"
     ]
    }
   ],
   "source": [
    "# Number of iterations\n",
    "num_requests = 200\n",
    "# Initial value to start 1.\n",
    "from_val = 1\n",
    "\n",
    "csv_file = 'output.csv'\n",
    "\n",
    "create_csv_file(csv_file)\n",
    "\n",
    "for i in range(num_requests):\n",
    "    make_request_and_append_to_csv(from_val, 'output.csv')\n",
    "    from_val += 1000  \n",
    "    time.sleep(5)\n",
    "    print(f\"Completed: {i} / {num_requests} , Index value : {from_val}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c74c5d48-62bf-4960-8213-b27af0dbaedd",
   "metadata": {},
   "source": [
    "### We can now filter and find submissions of interest\n",
    "\n",
    "**We've fetched all problems from contestid : 1928**\n",
    "\n",
    "Each contest contains several problems. For this contest, there are the following problems : \n",
    "\n",
    "- **A : Rectangle Cutting** (20,553 users attempted)\n",
    "- **B : Equalize** (14,643 users attempted)\n",
    "- **C : Physical education lesson** (7066 users attempted)\n",
    "- **D : Lonely Mountain Dungeons** (3588 users attempted)\n",
    "- **E : Modular Sequence** (1418 users attempted)\n",
    "- **F : Digital Patterns** (193 users attempted)\n",
    "  \n",
    "Now I filter by the problem and look for individuals that made multiple submissions for a given problem. In order to be considered, they must have a correct submissions (no compilation errors or wrong answer) and be written in C++ language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2840c413-bf0a-40d5-8ae0-908cb7428bcc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Adjust CSV reading parameters\n",
    "df = pd.read_csv('output.csv')\n",
    "# Remove entries with testset value 'WRONG_ANSWER'\n",
    "df = df[df['verdict'] != 'WRONG_ANSWER']\n",
    "df = df[df['verdict'] != 'COMPILATION_ERROR']\n",
    "df = df[df['verdict'] != 'RUNTIME_ERROR'] \n",
    "df = df[df['programming_language'].str.contains('C++')]\n",
    "df = df[df['problem_name'] == 'Equalize']\n",
    "\n",
    "#Remove authors that only submitted once.\n",
    "author_counts = df['author'].value_counts()\n",
    "multiple_submissions_authors = author_counts[author_counts > 1].index\n",
    "\n",
    "# Filter the DataFrame to only keep elements where the author appears more than once\n",
    "filtered_df = df[df['author'].isin(multiple_submissions_authors)]\n",
    "\n",
    "filtered_df.to_csv('filtered_output.csv', index=False)\n",
    "\n",
    "#Retrieve submissions ids\n",
    "id_values = filtered_df['id'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e978cec8-e455-4c4d-8453-79a78e1fce14",
   "metadata": {},
   "source": [
    "### Now we fetch the code for all these submissions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f27a51a-ed77-48db-9b80-c9ea7e2a4f4d",
   "metadata": {},
   "source": [
    "To fetch the submissions, we create a webscraper. The website's DDoS protection eventually kicks in so we implement several strategies : \n",
    "- Randomized access times [11, 20] seconds\n",
    "- Randomized user agent property to make it seem like multiple users are accessing from same IP.\n",
    "- Selenium web driver rather than requests. This is necessary to load Javascript and prevent detection, as requests does not load Javascript and therefore makes it easy to detect. \n",
    "- Rotating proxy addresses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c773bb-7e50-446b-ad8d-01d29fd6fccc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install selenium\n",
    "!pip install random_user_agent\n",
    "!pip install lxml\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "from random_user_agent.user_agent import UserAgent\n",
    "from random_user_agent.params import SoftwareName, OperatingSystem\n",
    "from selenium.webdriver.common.proxy import Proxy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f2663d5-b553-464e-a7fe-bc9083f2ff8f",
   "metadata": {},
   "source": [
    "This fetches a list of usable proxy addresses, updated every ten minutes. We can execute this function every ten minutes and fetch all the new addresses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3da97f7d-8b1a-475c-8b41-73dbeb6cd179",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def fetch_proxy_urls(driver):\n",
    "    driver.get('https://www.sslproxies.org/')\n",
    "    \n",
    "    # Extract the HTML content of the page\n",
    "    page_source = driver.page_source\n",
    "\n",
    "    # Parse the HTML content using BeautifulSoup\n",
    "    soup = BeautifulSoup(page_source, 'html.parser')\n",
    "\n",
    "    # Find the table containing proxy information\n",
    "    proxy_table = soup.find('table', class_='table')\n",
    "\n",
    "    # Extract IP addresses and ports from the table rows\n",
    "    proxy_server_urls = []\n",
    "    if proxy_table:\n",
    "        rows = proxy_table.find_all('tr')\n",
    "        for row in rows[1:]:  # Skip the header row\n",
    "            columns = row.find_all('td')\n",
    "            ip_address = columns[0].text\n",
    "            port = columns[1].text\n",
    "            proxy_url = f\"{ip_address}:{port}\"\n",
    "            proxy_server_urls.append(proxy_url)\n",
    "    driver.quit()\n",
    "    return proxy_server_urls\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a32a5c6-f2e2-4fc0-8a8e-c26ebfef4ddb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the directory to store code files\n",
    "directory = \"code_files\"\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "\n",
    "# Initialize UserAgent object\n",
    "software_names = [SoftwareName.EDGE.value, SoftwareName.CHROME.value, SoftwareName.CHROMIUM.value, SoftwareName.ANDROID.value, SoftwareName.FIREFOX.value, SoftwareName.OPERA.value, SoftwareName.SAFARI.value]\n",
    "operating_systems = [OperatingSystem.WINDOWS.value, OperatingSystem.LINUX.value, OperatingSystem.MAC.value]\n",
    "user_agent_rotator = UserAgent(software_names=software_names, operating_systems=operating_systems, limit=100)\n",
    "\n",
    "#chrome_options = Options()\n",
    "#chrome_options.add_argument(f'user-agent={user_agent}')\n",
    "#driver = webdriver.Chrome(options=chrome_options)\n",
    "#proxy_server_urls = fetch_proxy_urls(driver)\n",
    "# Iterate over each submission ID\n",
    "for submission_id in id_values:\n",
    "    try:\n",
    "        # Randomizing sleep duration (Must be greater than 10 to prevent DDOS protection from kicking in)\n",
    "        #if count % 35 == 0:\n",
    "        #    proxy_server_urls = fetch_proxy_urls(driver)\n",
    "        if os.path.isfile(file_name):\n",
    "            print(f\"File '{file_name}' exists.\")\n",
    "            continue\n",
    "        # URL of the submission page\n",
    "        submission_url = f\"https://codeforces.com/contest/1928/submission/{submission_id}\"\n",
    "\n",
    "        # Set a random User-Agent for each request\n",
    "        user_agent = user_agent_rotator.get_random_user_agent()\n",
    "          \n",
    "        # Set up Chrome options\n",
    "        chrome_options = Options()\n",
    "\n",
    "        # Set proxy server URL\n",
    "        #PROXY = proxy_server_urls[random.randint(0, len(proxy_server_urls) - 1)]\n",
    "        #print(PROXY)\n",
    "        # Add user agent, incognito mode, and headless mode to Chrome options\n",
    "        chrome_options.add_argument(f'user-agent={user_agent}')\n",
    "        chrome_options.add_argument(\"--incognito\")\n",
    "        chrome_options.add_argument(\"--headless\")\n",
    "\n",
    "        # Set proxy server for Chrome WebDriver\n",
    "        #chrome_options.add_argument(\"--proxy-server=%s\" % PROXY)\n",
    "\n",
    "        # Initialize Chrome WebDriver with options\n",
    "        driver = webdriver.Chrome(options=chrome_options)\n",
    "\n",
    "        # Navigate to the submission page\n",
    "        driver.get(submission_url)\n",
    "        time.sleep(random.randint(9, 12))\n",
    "        # Extract the HTML content of the page\n",
    "        page_source = driver.page_source\n",
    "        \n",
    "        # Parse the HTML content using BeautifulSoup\n",
    "        soup = BeautifulSoup(page_source, 'lxml')\n",
    "\n",
    "        not_allowed_messages = soup.find_all(string='403 Forbidden')\n",
    "\n",
    "        if not_allowed_messages:\n",
    "            print(f\"Scraping detected.\")\n",
    "            time.sleep(random.randint(400, 500))\n",
    "            continue\n",
    "        \n",
    "        # Extract the code from the page\n",
    "        code_element = soup.find('pre', class_='prettyprint')\n",
    "        \n",
    "        if code_element:\n",
    "            code = code_element.get_text()\n",
    "\n",
    "            # Write the code to a .txt file\n",
    "            file_path = os.path.join(directory, f\"submission_{submission_id}.txt\")\n",
    "            with open(file_path, \"w\", encoding=\"utf-8\") as file:\n",
    "                file.write(code)\n",
    "            print(f\"Code found for submission {submission_id}.\")\n",
    "        else:\n",
    "            print(f\"No code found for submission {submission_id}.\")    \n",
    "        time.sleep(5)\n",
    "        driver.quit()\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred for submission {submission_id}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45d09e5b-cec8-4eeb-b364-4749161b82aa",
   "metadata": {},
   "source": [
    "## Creating Bag of Words model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7620baa6-c403-4778-ba2f-b1085903552e",
   "metadata": {},
   "source": [
    "#### Pre-processing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "629ad3c7-f4a1-420e-b8fa-b01bb4184f5d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install nltk\n",
    "!pip install sctokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "215d8203-6fc9-4285-ae98-f29cc83e5e06",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from sctokenizer import CppTokenizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "model_csv\n",
    "\n",
    "def preprocess_code(code):\n",
    "    # Replace \";\" with newline character\n",
    "    code = code.replace(';', ';\\n')\n",
    "    return code\n",
    "\n",
    "def tokenize_cpp_file(file_path):\n",
    "    tokenizer = CppTokenizer()\n",
    "    with open(file_path) as file:\n",
    "        source = file.read()\n",
    "        # Tokenize preprocessed code\n",
    "        code = preprocess_code(source)\n",
    "        tokens = tokenizer.tokenize(code)\n",
    "        return ' '.join(token.token_value for token in tokens)  # Join tokens into a single string\n",
    "\n",
    "def process_code_files(directory):\n",
    "    corpus = []  # List to store tokenized code from all files\n",
    "    for file_name in os.listdir(directory):\n",
    "        if file_name.endswith(\".txt\"):  # Process only .txt files\n",
    "            file_path = os.path.join(directory, file_name)\n",
    "            tokens = tokenize_cpp_file(file_path)\n",
    "            corpus.append(tokens)\n",
    "\n",
    "    # Apply TF-IDF processing\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = vectorizer.fit_transform(corpus)\n",
    "\n",
    "    return vectorizer, tfidf_matrix\n",
    "\n",
    "directory = \"code_files\"\n",
    "vectorizer, tfidf_matrix = process_code_files(directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aef5ca5-dd94-466b-8c41-56c41ee7b87c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sctokenizer import CppTokenizer\n",
    "\n",
    "def preprocess_code(code):\n",
    "    code = code.replace(';', ';\\n')\n",
    "    return code\n",
    "\n",
    "def tokenize_cpp_file(file_path):\n",
    "    tokenizer = CppTokenizer()\n",
    "    with open(file_path) as file:\n",
    "        source = file.read()\n",
    "        code = preprocess_code(source)\n",
    "        tokens = tokenizer.tokenize(code)\n",
    "        return ','.join(token.token_value for token in tokens)\n",
    "\n",
    "def process_code_files(directory, output_csv):\n",
    "    df_list = []\n",
    "\n",
    "    for file_name in os.listdir(directory):\n",
    "        if file_name.endswith(\".txt\"):\n",
    "            submission_id = file_name.split('_')[-1].split('.')[0]\n",
    "\n",
    "            file_path = os.path.join(directory, file_name)\n",
    "            tokens = tokenize_cpp_file(file_path)\n",
    "\n",
    "            df_list.append({'id': submission_id,\n",
    "                            'tokens': tokens})\n",
    "\n",
    "    df = pd.DataFrame(df_list)\n",
    "\n",
    "    output_df = pd.read_csv(output_csv)\n",
    "    output_df = output_df[['id', 'time_consumed_millis', 'memory_consumed_bytes']]\n",
    "\n",
    "  \n",
    "    df['id'] = df['id'].astype(str)\n",
    "    output_df['id'] = output_df['id'].astype(str)\n",
    "\n",
    "\n",
    "    df = pd.merge(df, output_df, on='id', how='left')\n",
    "\n",
    "    # Save final_df to CSV\n",
    "    final_df.to_csv('model_data.csv', index=False)\n",
    "\n",
    "directory = \"code_files\"\n",
    "output_csv = \"output.csv\"\n",
    "process_code_files(directory, output_csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34289ff5-4b17-4967-ba0c-5c9eb16e97af",
   "metadata": {},
   "source": [
    "## Constructing the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30a4a6cf-d696-426f-b61f-6468aaed7328",
   "metadata": {},
   "source": [
    "We're going to make 2 models : \n",
    "1. Execution time\n",
    "2. Memory consumption"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48d1e1f6-e0f1-4edd-9fe0-41b6fadfca9f",
   "metadata": {},
   "source": [
    "#### Splitting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2937adbc-e3cf-44e8-ae9d-4dc333baa9b9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "df = pd.read_csv('model_data.csv')\n",
    " \n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f2dcca7-617e-40ad-8c5f-627c4fac4e17",
   "metadata": {},
   "source": [
    "#### Apply TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "72d584f5-3644-4e01-9467-e37c844bac69",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "corpus = []\n",
    "\n",
    "for index, row in train_df.iterrows():\n",
    "    tokens = row['tokens']\n",
    "    corpus.append(' '.join(tokens.split()))\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "tfidf_matrix = vectorizer.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0452558e-3962-4fad-9ce9-ed8ff4f1ce63",
   "metadata": {},
   "source": [
    "Applying TF-IDF to test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "81360192-ad31-4d97-b123-3e8ce2902e20",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_corpus = []\n",
    "\n",
    "for index, row in test_df.iterrows():\n",
    "    tokens = row['tokens']\n",
    "    test_corpus.append(' '.join(tokens.split()))\n",
    "\n",
    "test_tfidf_matrix = vectorizer.transform(test_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "247fcde3-4d4a-43fb-878b-541c8a7c6e3e",
   "metadata": {},
   "source": [
    "### Baseline Model - Time consumed in milliseconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c21cd57-b454-45b6-aa85-04863308199f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.dummy import DummyRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "dummy_regressor = DummyRegressor(strategy='mean')\n",
    "dummy_regressor.fit(tfidf_matrix, train_df['time_consumed_millis'])\n",
    "\n",
    "predicted_values = dummy_regressor.predict(test_tfidf_matrix)\n",
    "\n",
    "mse = mean_squared_error(test_df['time_consumed_millis'], predicted_values)\n",
    "print(\"Root Mean Squared Error (RMSE) of the baseline model:\", np.sqrt(mse))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe749072-2fe2-4fef-9714-ead9dab66823",
   "metadata": {},
   "source": [
    "Baseline model : Mean Squared Error (MSE) of the baseline model: 394.7721080615323"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "370addcc-e36b-43f9-b688-ae7c3f47d75b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "mlpreg = MLPRegressor(hidden_layer_sizes=(tfidf_matrix.shape[0],), alpha=0.01, max_iter=1000, random_state=42)\n",
    "mlpreg.fit(tfidf_matrix, train_df['time_consumed_millis'])\n",
    "\n",
    "predicted_values = mlpreg.predict(test_tfidf_matrix)\n",
    "\n",
    "mse = mean_squared_error(test_df['time_consumed_millis'], predicted_values)\n",
    "print(\"Root Mean Squared Error (RMSE) of the baseline model:\", np.sqrt(mse))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61808bf7-2944-40d1-8e68-1cd04381b312",
   "metadata": {},
   "source": [
    "Untuned model : Mean Squared Error (MSE) of the baseline model: 362.25048999443374"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40d76b9a-2283-4115-a8e7-22561911e72c",
   "metadata": {},
   "source": [
    "#### Gridsearch on MLPRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a292393-9f38-4763-a1cc-beaa4b578865",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "param_grid = {\n",
    "    'hidden_layer_sizes': [(50,), (100,), (200,), (300,), (tfidf_matrix.shape[0],)],\n",
    "    'alpha': [ 0.01, 0.1, 0.5], \n",
    "    'max_iter': [1500], \n",
    "    'solver': ['adam'], \n",
    "    'activation': ['logistic'] \n",
    "}\n",
    "\n",
    "mlpreg = MLPRegressor(random_state=42)\n",
    "grid_search = GridSearchCV(mlpreg, param_grid, cv=5, scoring='neg_mean_squared_error')\n",
    "grid_search.fit(tfidf_matrix, train_df['time_consumed_millis'])\n",
    "\n",
    "best_params = grid_search.best_params_\n",
    "print(\"Best hyperparameters:\", best_params)\n",
    "\n",
    "best_model = grid_search.best_estimator_\n",
    "predicted_values = best_model.predict(test_tfidf_matrix)\n",
    "\n",
    "mse = mean_squared_error(test_df['time_consumed_millis'], predicted_values)\n",
    "rmse = np.sqrt(mse)\n",
    "print(\"Root Mean Squared Error (RMSE) of the best model:\", rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "318f38ce-190d-42eb-b324-079f8fa52d80",
   "metadata": {},
   "source": [
    "Best hyperparameters: {'activation': 'logistic', 'alpha': 0.5, 'hidden_layer_sizes': (439,), 'max_iter': 1500, 'solver': 'adam'}\n",
    "\n",
    "Tuned model : Root Mean Squared Error (RMSE) of the best model: 358.45408676412364"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b1ae43-bf5f-4e80-bb18-48240a920205",
   "metadata": {},
   "source": [
    "### Observations\n",
    "Current error is quite high. This is indicative that either our data either has errors or our model has low predictive abilities. I found in the data that we were conserving runtime errors which made it so we had skewed outputs for time in milliseconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "318236ce-35cb-4a28-b92a-3ec29bbb8f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "logit_reg = LogisticRegression(random_state=42, penalty=\"l2\", C=0.5)\n",
    "logit_reg.fit(tfidf_matrix, train_df['time_consumed_millis'])\n",
    "predicted_values = logit_reg.predict(test_tfidf_matrix)\n",
    "mse = mean_squared_error(test_df['time_consumed_millis'], predicted_values)\n",
    "rmse = np.sqrt(mse)\n",
    "print(\"Root Mean Squared Error (RMSE) of the Logistic Regression model:\",rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5ed66b2-24da-4e2b-a4e5-f09ba85f86f5",
   "metadata": {},
   "source": [
    "This has worst predictive abilities than a dummy regressor.\n",
    "\n",
    "Root Mean Squared Error (RMSE) of the Logistic Regression model: 613.6671069739832"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4358ba1b-a30a-4fa7-8cd4-82c556f4bc7b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
