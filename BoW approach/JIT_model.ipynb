{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e6db105-253c-4b41-b2e2-97fe008f31c7",
   "metadata": {},
   "source": [
    "### Finding the main method of a java file and adding @Benchmark annotation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c793dd34-b46c-4836-b489-b6f79715ed0b",
   "metadata": {},
   "source": [
    "This program will iterate over every file in the directory and add @Benchmark tag over each function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "204ff9aa-0cfb-44cc-8e86-20d35c3caa81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed annotation for file : ForwardBackward.java \n",
      "Completed annotation for file : Recursive.java \n",
      "Completed annotation for file : Shifting.java \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "# Define the folder containing Java files\n",
    "folder_path = r'C:\\Users\\Mettle\\Desktop\\Java-master\\java-combinations\\src\\main\\java\\com\\hmkcode'\n",
    "\n",
    "# Create the 'benchmark_files' folder if it doesn't exist\n",
    "if not os.path.exists('benchmark_files'):\n",
    "    os.makedirs('benchmark_files')\n",
    "\n",
    "# Function to process each Java file\n",
    "def process_java_file(java_file_path):\n",
    "    benchmark_file_path = os.path.join('benchmark_files', os.path.basename(java_file_path))\n",
    "    add_benchmark_annotation(java_file_path, benchmark_file_path)\n",
    "    add_counters(benchmark_file_path)\n",
    "    method_invocation_annotation(benchmark_file_path)\n",
    "   \n",
    "# Iterate over each file in the folder\n",
    "for file_name in os.listdir(folder_path):\n",
    "    if file_name.endswith('.java'):\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "        process_java_file(file_path)\n",
    "        print(f\"Completed annotation for file : {file_name} \")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cab5b9a-2d4b-49f9-b3ae-0c050d45e39c",
   "metadata": {},
   "source": [
    "#### Loop counter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c553d0ad-1976-4a90-be21-bb00cf18b5b0",
   "metadata": {},
   "source": [
    "*Currently not handling case with loops without braces.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d1fdf559-bb59-4e5a-8244-713c196ec384",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_counters(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "            java_code = file.read()\n",
    "    # Regex pattern for finding for loops and while loops\n",
    "    pattern = r'(?P<loop>(for\\s*\\([^\\)]*\\)\\s*\\{)|(while\\s*\\([^\\)]*\\)\\s*\\{))'\n",
    "    \n",
    "    def add_counter(match):\n",
    "        loop = match.group('loop')\n",
    "        return loop + '\\n    counter++;'\n",
    "    \n",
    "    # Replace each loop with the loop plus counter++\n",
    "    modified_java_code = re.sub(pattern, add_counter, java_code)\n",
    "    with open(file_path, 'w') as file:\n",
    "        file.write(modified_java_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8afbe892-3588-4713-b559-34fc72f244aa",
   "metadata": {},
   "source": [
    "#### @Benchmark Annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9a0d1066-9a92-4f1c-a1e2-03b2382451fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_benchmark_annotation(file_path, benchmark_file_path):\n",
    "    pattern = r'(?<!\\/\\/)(?<!\\/\\*)\\b(?:public\\s+|private\\s+|protected\\s+|static\\s+|final\\s+|native\\s+|synchronized\\s+|abstract\\s+|transient\\s+)*[\\$_\\w<>\\[\\]]*\\s+\\w+\\s*\\([^\\)]*\\)?\\s*\\{'\n",
    "\n",
    "    with open(file_path, 'r') as file:\n",
    "        java_code = file.read()\n",
    "\n",
    "    # Find all occurrences of method declarations in the Java code\n",
    "    matches = re.finditer(pattern, java_code)\n",
    "\n",
    "    modified_java_code = ''\n",
    "    previous_end_index = 0\n",
    "\n",
    "    # Loop through each method occurrence\n",
    "    for match in matches:\n",
    "        start_index = match.start()\n",
    "        end_index = match.end()\n",
    "        \n",
    "        method_declaration = java_code[start_index:end_index]\n",
    "        \n",
    "        # Insert @Benchmark annotation just above the method declaration\n",
    "        modified_java_code += java_code[previous_end_index:start_index] + '@Benchmark\\n' + method_declaration\n",
    "\n",
    "        previous_end_index = end_index\n",
    "\n",
    "    modified_java_code += java_code[previous_end_index:]\n",
    "    with open(benchmark_file_path, 'w') as file:\n",
    "        file.write(modified_java_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "302ebcf6-df6c-493e-90a3-55709e87819b",
   "metadata": {},
   "source": [
    "#### Count method invocation values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "73787349-7e31-4974-94c2-00320c8eae95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def method_invocation_annotation(file_path):\n",
    "    method_pattern = r'(?P<method>(?<!\\/\\/)(?<!\\/\\*)\\b(?:public\\s+|private\\s+|protected\\s+|static\\s+|final\\s+|native\\s+|synchronized\\s+|abstract\\s+|transient\\s+)*[\\$_\\w<>\\[\\]]*\\s+\\w+\\s*\\([^\\)]*\\)\\s*\\{[^\\}]*?\\})'\n",
    "    with open(file_path, 'r') as file:\n",
    "        java_code = file.read()\n",
    "    def add_counter(match):\n",
    "        method = match.group()\n",
    "        modified_method = method.replace('{', '{\\n    method_counter++;\\n')\n",
    "        return modified_method\n",
    "\n",
    "    # Replace each method with additional code added inside\n",
    "    modified_java_code = re.sub(method_pattern, add_counter, java_code)\n",
    "    with open(file_path, 'w') as file:\n",
    "        file.write(modified_java_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e131558-8d65-418e-aa71-8814bcd2ecbe",
   "metadata": {},
   "source": [
    "# Getting Codeforces submissions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7c79329-86bd-432a-8de9-60144297e090",
   "metadata": {},
   "source": [
    "### Generate URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d0bfd351-d379-4df9-a216-4e0d84b930c8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import hashlib\n",
    "import time\n",
    "import random\n",
    "from bs4 import BeautifulSoup\n",
    "def get_url(from_val):\n",
    "    key = \"baa5c566fa5bdeb92494876e5bcac06b6798d8fe\"\n",
    "    secret = \"0d686bfc8a3c654a569b93e3d1d12803d25264f1\"\n",
    "    count = 1000\n",
    "    current_time = int(time.time())\n",
    "    random_number = random.randint(100000, 999999)\n",
    "    method_fetched = \"contest.status\"\n",
    "    string_to_hash = f\"{random_number}/{method_fetched}?apiKey={key}&contestId=1928&count={count}&from={from_val}&time={current_time}#{secret}\"\n",
    "    sha512_hash = hashlib.sha512(string_to_hash.encode()).hexdigest()\n",
    "    req_url = f\"https://codeforces.com/api/{method_fetched}?apiKey={key}&contestId=1928&count={count}&from={from_val}&time={current_time}&apiSig={random_number}{sha512_hash}\"\n",
    "    return req_url"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd8cd6ee-50a2-4773-b9de-9bd2ab0afdb8",
   "metadata": {},
   "source": [
    "### Next step is to make a crawler to incrementally fetch data from Codeforces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8ce05d3b-bb1d-4f12-8a55-94bb39875841",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import os.path\n",
    "from os import path\n",
    "\n",
    "# Function to check if file exists and create it if it doesn't\n",
    "def create_csv_file(file_path):\n",
    "    if not path.exists(file_path):\n",
    "        with open(file_path, 'w', newline='') as file:\n",
    "            # Create a CSV writer object\n",
    "            csv_writer = csv.writer(file)\n",
    "            # Write header row\n",
    "            csv_writer.writerow(['id', 'contest_id', 'author','creation_time_seconds', 'relative_time_seconds', 'problem_name', 'problem_type', 'programming_language', 'verdict', 'test_set', 'passed_test_count', 'time_consumed_millis', 'memory_consumed_bytes'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "d7409cde-548f-4055-b8a5-55c483b2330d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_to_csv(data, file_path):\n",
    "    fieldnames = ['id', 'contest_id', 'author','creation_time_seconds', 'relative_time_seconds', 'problem_name', 'problem_type', 'programming_language', 'verdict', 'test_set', 'passed_test_count', 'time_consumed_millis', 'memory_consumed_bytes']\n",
    "    \n",
    "    file_exists = os.path.isfile(file_path)\n",
    "    \n",
    "    with open(file_path, 'a', newline='') as file:\n",
    "        writer = csv.DictWriter(file, fieldnames=fieldnames)\n",
    "        if not file_exists:\n",
    "            writer.writeheader()  # Write header only if file is new\n",
    "        writer.writerow(data)\n",
    "\n",
    "def make_request_and_append_to_csv(from_val, file_path):\n",
    "    req_url = get_url(from_val)\n",
    "    response = requests.get(req_url)\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        for item in data['result']:\n",
    "            csv_data = {\n",
    "                'id': item['id'],\n",
    "                'contest_id': item['contestId'],\n",
    "                'author': item['author']['members'][0]['handle'],\n",
    "                'creation_time_seconds': item['creationTimeSeconds'],\n",
    "                'relative_time_seconds': item['relativeTimeSeconds'],\n",
    "                'problem_name': item['problem']['name'],\n",
    "                'problem_type': item['problem']['type'],\n",
    "                'programming_language': item['programmingLanguage'],\n",
    "                'verdict': item['verdict'],\n",
    "                'test_set': item['testset'],\n",
    "                'passed_test_count': item['passedTestCount'],\n",
    "                'time_consumed_millis': item['timeConsumedMillis'],\n",
    "                'memory_consumed_bytes': item['memoryConsumedBytes']\n",
    "            }\n",
    "            append_to_csv(csv_data, file_path)\n",
    "    else:\n",
    "        print(\"Request failed with status code:\", response.status_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2232d031-4501-456e-bdb5-e0dd4dc97aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of iterations\n",
    "num_requests = 200\n",
    "# Initial value to start 1.\n",
    "from_val = 1\n",
    "\n",
    "csv_file = 'output.csv'\n",
    "\n",
    "create_csv_file(csv_file)\n",
    "\n",
    "for i in range(num_requests):\n",
    "    make_request_and_append_to_csv(from_val, 'output.csv')\n",
    "    from_val += 1000  \n",
    "    time.sleep(5)\n",
    "    print(f\"Completed: {i} / {num_requests} , Index value : {from_val}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c74c5d48-62bf-4960-8213-b27af0dbaedd",
   "metadata": {},
   "source": [
    "### We can now filter and find submissions of interest\n",
    "\n",
    "**We've fetched all problems from contestid : 1928**\n",
    "\n",
    "Each contest contains several problems. For this contest, there are the following problems : \n",
    "\n",
    "- **A : Rectangle Cutting** (20,553 users attempted)\n",
    "- **B : Equalize** (14,643 users attempted)\n",
    "- **C : Physical education lesson** (7066 users attempted)\n",
    "- **D : Lonely Mountain Dungeons** (3588 users attempted)\n",
    "- **E : Modular Sequence** (1418 users attempted)\n",
    "- **F : Digital Patterns** (193 users attempted)\n",
    "  \n",
    "Now I filter by the problem and look for individuals that made multiple submissions for a given problem. In order to be considered, they must have a correct submissions (no compilation errors or wrong answer) and be written in C++ language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2840c413-bf0a-40d5-8ae0-908cb7428bcc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5368\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Adjust CSV reading parameters\n",
    "df = pd.read_csv('output.csv')\n",
    "# Remove entries with testset value 'WRONG_ANSWER'\n",
    "df = df[df['verdict'] != 'WRONG_ANSWER']\n",
    "df = df[df['verdict'] != 'COMPILATION_ERROR']\n",
    "df = df[df['verdict'] != 'RUNTIME_ERROR'] \n",
    "df = df[df['programming_language'].str.contains('C++')]\n",
    "df = df[df['problem_name'] == 'Equalize']\n",
    "\n",
    "#Remove authors that only submitted once.\n",
    "author_counts = df['author'].value_counts()\n",
    "multiple_submissions_authors = author_counts[author_counts > 1].index\n",
    "\n",
    "# Filter the DataFrame to only keep elements where the author appears more than once\n",
    "filtered_df = df[df['author'].isin(multiple_submissions_authors)]\n",
    "\n",
    "filtered_df.to_csv('filtered_output.csv', index=False)\n",
    "\n",
    "#Retrieve submissions ids\n",
    "id_values = filtered_df['id'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e978cec8-e455-4c4d-8453-79a78e1fce14",
   "metadata": {},
   "source": [
    "### Now we fetch the code for all these submissions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f27a51a-ed77-48db-9b80-c9ea7e2a4f4d",
   "metadata": {},
   "source": [
    "To fetch the submissions, we create a webscraper. The website's DDoS protection eventually kicks in so we implement several strategies : \n",
    "- Randomized access times [11, 20] seconds\n",
    "- Randomized user agent property to make it seem like multiple users are accessing from same IP.\n",
    "- Selenium web driver rather than requests. This is necessary to load Javascript and prevent detection, as requests does not load Javascript and therefore makes it easy to detect. \n",
    "- Rotating proxy addresses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c773bb-7e50-446b-ad8d-01d29fd6fccc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install selenium\n",
    "!pip install random_user_agent\n",
    "!pip install lxml\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "from random_user_agent.user_agent import UserAgent\n",
    "from random_user_agent.params import SoftwareName, OperatingSystem\n",
    "from selenium.webdriver.common.proxy import Proxy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f2663d5-b553-464e-a7fe-bc9083f2ff8f",
   "metadata": {},
   "source": [
    "This fetches a list of usable proxy addresses, updated every ten minutes. We can execute this function every ten minutes and fetch all the new addresses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3da97f7d-8b1a-475c-8b41-73dbeb6cd179",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def fetch_proxy_urls(driver):\n",
    "    driver.get('https://www.sslproxies.org/')\n",
    "    \n",
    "    # Extract the HTML content of the page\n",
    "    page_source = driver.page_source\n",
    "\n",
    "    # Parse the HTML content using BeautifulSoup\n",
    "    soup = BeautifulSoup(page_source, 'html.parser')\n",
    "\n",
    "    # Find the table containing proxy information\n",
    "    proxy_table = soup.find('table', class_='table')\n",
    "\n",
    "    # Extract IP addresses and ports from the table rows\n",
    "    proxy_server_urls = []\n",
    "    if proxy_table:\n",
    "        rows = proxy_table.find_all('tr')\n",
    "        for row in rows[1:]:  # Skip the header row\n",
    "            columns = row.find_all('td')\n",
    "            ip_address = columns[0].text\n",
    "            port = columns[1].text\n",
    "            proxy_url = f\"{ip_address}:{port}\"\n",
    "            proxy_server_urls.append(proxy_url)\n",
    "    driver.quit()\n",
    "    return proxy_server_urls\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a32a5c6-f2e2-4fc0-8a8e-c26ebfef4ddb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Code found for submission 253261226.\n",
      "Code found for submission 253228787.\n",
      "No code found for submission 253227477.\n",
      "Code found for submission 253224124.\n",
      "Code found for submission 253223003.\n",
      "No code found for submission 253222689.\n",
      "No code found for submission 253222544.\n"
     ]
    }
   ],
   "source": [
    "# Define the directory to store code files\n",
    "directory = \"code_files\"\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "\n",
    "# Initialize UserAgent object\n",
    "software_names = [SoftwareName.EDGE.value, SoftwareName.CHROME.value, SoftwareName.CHROMIUM.value, SoftwareName.ANDROID.value, SoftwareName.FIREFOX.value, SoftwareName.OPERA.value, SoftwareName.SAFARI.value]\n",
    "operating_systems = [OperatingSystem.WINDOWS.value, OperatingSystem.LINUX.value, OperatingSystem.MAC.value]\n",
    "user_agent_rotator = UserAgent(software_names=software_names, operating_systems=operating_systems, limit=100)\n",
    "\n",
    "#chrome_options = Options()\n",
    "#chrome_options.add_argument(f'user-agent={user_agent}')\n",
    "#driver = webdriver.Chrome(options=chrome_options)\n",
    "#proxy_server_urls = fetch_proxy_urls(driver)\n",
    "count = 0\n",
    "# Iterate over each submission ID\n",
    "for submission_id in id_values:\n",
    "    try:\n",
    "        # Randomizing sleep duration (Must be greater than 10 to prevent DDOS protection from kicking in)\n",
    "        #if count % 35 == 0:\n",
    "        #    proxy_server_urls = fetch_proxy_urls(driver)\n",
    "     \n",
    "        # URL of the submission page\n",
    "        submission_url = f\"https://codeforces.com/contest/1928/submission/{submission_id}\"\n",
    "\n",
    "        # Set a random User-Agent for each request\n",
    "        user_agent = user_agent_rotator.get_random_user_agent()\n",
    "          \n",
    "        # Set up Chrome options\n",
    "        chrome_options = Options()\n",
    "\n",
    "        # Set proxy server URL\n",
    "        #PROXY = proxy_server_urls[random.randint(0, len(proxy_server_urls) - 1)]\n",
    "        #print(PROXY)\n",
    "        # Add user agent, incognito mode, and headless mode to Chrome options\n",
    "        chrome_options.add_argument(f'user-agent={user_agent}')\n",
    "        chrome_options.add_argument(\"--incognito\")\n",
    "        chrome_options.add_argument(\"--headless\")\n",
    "\n",
    "        # Set proxy server for Chrome WebDriver\n",
    "        #chrome_options.add_argument(\"--proxy-server=%s\" % PROXY)\n",
    "\n",
    "        # Initialize Chrome WebDriver with options\n",
    "        driver = webdriver.Chrome(options=chrome_options)\n",
    "\n",
    "        # Navigate to the submission page\n",
    "        driver.get(submission_url)\n",
    "        \n",
    "        # Extract the HTML content of the page\n",
    "        page_source = driver.page_source\n",
    "        \n",
    "        # Parse the HTML content using BeautifulSoup\n",
    "        soup = BeautifulSoup(page_source, 'lxml')\n",
    "\n",
    "        # Extract the code from the page\n",
    "        code_element = soup.find('pre', class_='prettyprint')\n",
    "        \n",
    "        if code_element:\n",
    "            code = code_element.get_text()\n",
    "\n",
    "            # Write the code to a .txt file\n",
    "            file_path = os.path.join(directory, f\"submission_{submission_id}.txt\")\n",
    "            with open(file_path, \"w\", encoding=\"utf-8\") as file:\n",
    "                file.write(code)\n",
    "            print(f\"Code found for submission {submission_id}.\")\n",
    "            count = 0\n",
    "        else:\n",
    "            print(f\"No code found for submission {submission_id}.\")\n",
    "            time.sleep(200 + (count * 180))\n",
    "            count = count + 1\n",
    "            if count > 3:\n",
    "                driver.quit()\n",
    "                break\n",
    "        \n",
    "        time.sleep(random.randint(18, 22))\n",
    "        driver.quit()\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred for submission {submission_id}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45d09e5b-cec8-4eeb-b364-4749161b82aa",
   "metadata": {},
   "source": [
    "## Creating Bag of Words model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7620baa6-c403-4778-ba2f-b1085903552e",
   "metadata": {},
   "source": [
    "#### Pre-processing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "629ad3c7-f4a1-420e-b8fa-b01bb4184f5d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install nltk\n",
    "!pip install sctokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "215d8203-6fc9-4285-ae98-f29cc83e5e06",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from sctokenizer import CppTokenizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "model_csv\n",
    "\n",
    "def preprocess_code(code):\n",
    "    # Replace \";\" with newline character\n",
    "    code = code.replace(';', ';\\n')\n",
    "    return code\n",
    "\n",
    "def tokenize_cpp_file(file_path):\n",
    "    tokenizer = CppTokenizer()\n",
    "    with open(file_path) as file:\n",
    "        source = file.read()\n",
    "        # Tokenize preprocessed code\n",
    "        code = preprocess_code(source)\n",
    "        tokens = tokenizer.tokenize(code)\n",
    "        return ' '.join(token.token_value for token in tokens)  # Join tokens into a single string\n",
    "\n",
    "def process_code_files(directory):\n",
    "    corpus = []  # List to store tokenized code from all files\n",
    "    for file_name in os.listdir(directory):\n",
    "        if file_name.endswith(\".txt\"):  # Process only .txt files\n",
    "            file_path = os.path.join(directory, file_name)\n",
    "            tokens = tokenize_cpp_file(file_path)\n",
    "            corpus.append(tokens)\n",
    "\n",
    "    # Apply TF-IDF processing\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = vectorizer.fit_transform(corpus)\n",
    "\n",
    "    return vectorizer, tfidf_matrix\n",
    "\n",
    "directory = \"code_files\"\n",
    "vectorizer, tfidf_matrix = process_code_files(directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1aef5ca5-dd94-466b-8c41-56c41ee7b87c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sctokenizer import CppTokenizer\n",
    "\n",
    "def preprocess_code(code):\n",
    "    code = code.replace(';', ';\\n')\n",
    "    return code\n",
    "\n",
    "def tokenize_cpp_file(file_path):\n",
    "    tokenizer = CppTokenizer()\n",
    "    with open(file_path) as file:\n",
    "        source = file.read()\n",
    "        code = preprocess_code(source)\n",
    "        tokens = tokenizer.tokenize(code)\n",
    "        return ','.join(token.token_value for token in tokens)\n",
    "\n",
    "def process_code_files(directory, output_csv):\n",
    "    df_list = []\n",
    "\n",
    "    for file_name in os.listdir(directory):\n",
    "        if file_name.endswith(\".txt\"):\n",
    "            submission_id = file_name.split('_')[-1].split('.')[0]\n",
    "\n",
    "            file_path = os.path.join(directory, file_name)\n",
    "            tokens = tokenize_cpp_file(file_path)\n",
    "\n",
    "            df_list.append({'id': submission_id,\n",
    "                            'tokens': tokens})\n",
    "\n",
    "    df = pd.DataFrame(df_list)\n",
    "\n",
    "    output_df = pd.read_csv(output_csv)\n",
    "    output_df = output_df[['id', 'time_consumed_millis', 'memory_consumed_bytes']]\n",
    "\n",
    "  \n",
    "    df['id'] = df['id'].astype(str)\n",
    "    output_df['id'] = output_df['id'].astype(str)\n",
    "\n",
    "\n",
    "    df = pd.merge(df, output_df, on='id', how='left')\n",
    "\n",
    "    # Save final_df to CSV\n",
    "    final_df.to_csv('model_data.csv', index=False)\n",
    "\n",
    "directory = \"code_files\"\n",
    "output_csv = \"output.csv\"\n",
    "process_code_files(directory, output_csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34289ff5-4b17-4967-ba0c-5c9eb16e97af",
   "metadata": {},
   "source": [
    "## Constructing the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30a4a6cf-d696-426f-b61f-6468aaed7328",
   "metadata": {},
   "source": [
    "We're going to make 2 models : \n",
    "1. Execution time\n",
    "2. Memory consumption"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48d1e1f6-e0f1-4edd-9fe0-41b6fadfca9f",
   "metadata": {},
   "source": [
    "#### Splitting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2937adbc-e3cf-44e8-ae9d-4dc333baa9b9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "df = pd.read_csv('model_data.csv')\n",
    " \n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f2dcca7-617e-40ad-8c5f-627c4fac4e17",
   "metadata": {},
   "source": [
    "#### Apply TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "72d584f5-3644-4e01-9467-e37c844bac69",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "corpus = []\n",
    "\n",
    "for index, row in train_df.iterrows():\n",
    "    tokens = row['tokens']\n",
    "    corpus.append(' '.join(tokens.split()))\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "tfidf_matrix = vectorizer.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0452558e-3962-4fad-9ce9-ed8ff4f1ce63",
   "metadata": {},
   "source": [
    "Applying TF-IDF to test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "81360192-ad31-4d97-b123-3e8ce2902e20",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_corpus = []\n",
    "\n",
    "for index, row in test_df.iterrows():\n",
    "    tokens = row['tokens']\n",
    "    test_corpus.append(' '.join(tokens.split()))\n",
    "\n",
    "test_tfidf_matrix = vectorizer.transform(test_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "247fcde3-4d4a-43fb-878b-541c8a7c6e3e",
   "metadata": {},
   "source": [
    "### Baseline Model - Time consumed in milliseconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c21cd57-b454-45b6-aa85-04863308199f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.dummy import DummyRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "dummy_regressor = DummyRegressor(strategy='mean')\n",
    "dummy_regressor.fit(tfidf_matrix, train_df['time_consumed_millis'])\n",
    "\n",
    "predicted_values = dummy_regressor.predict(test_tfidf_matrix)\n",
    "\n",
    "mse = mean_squared_error(test_df['time_consumed_millis'], predicted_values)\n",
    "print(\"Root Mean Squared Error (RMSE) of the baseline model:\", np.sqrt(mse))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe749072-2fe2-4fef-9714-ead9dab66823",
   "metadata": {},
   "source": [
    "Baseline model : Mean Squared Error (MSE) of the baseline model: 394.7721080615323"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "370addcc-e36b-43f9-b688-ae7c3f47d75b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "mlpreg = MLPRegressor(hidden_layer_sizes=(tfidf_matrix.shape[0],), alpha=0.01, max_iter=1000, random_state=42)\n",
    "mlpreg.fit(tfidf_matrix, train_df['time_consumed_millis'])\n",
    "\n",
    "predicted_values = mlpreg.predict(test_tfidf_matrix)\n",
    "\n",
    "mse = mean_squared_error(test_df['time_consumed_millis'], predicted_values)\n",
    "print(\"Root Mean Squared Error (RMSE) of the baseline model:\", np.sqrt(mse))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61808bf7-2944-40d1-8e68-1cd04381b312",
   "metadata": {},
   "source": [
    "Untuned model : Mean Squared Error (MSE) of the baseline model: 362.25048999443374"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40d76b9a-2283-4115-a8e7-22561911e72c",
   "metadata": {},
   "source": [
    "#### Gridsearch on MLPRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a292393-9f38-4763-a1cc-beaa4b578865",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "param_grid = {\n",
    "    'hidden_layer_sizes': [(50,), (100,), (200,), (300,), (tfidf_matrix.shape[0],)],\n",
    "    'alpha': [ 0.01, 0.1, 0.5], \n",
    "    'max_iter': [1500], \n",
    "    'solver': ['adam'], \n",
    "    'activation': ['logistic'] \n",
    "}\n",
    "\n",
    "mlpreg = MLPRegressor(random_state=42)\n",
    "grid_search = GridSearchCV(mlpreg, param_grid, cv=5, scoring='neg_mean_squared_error')\n",
    "grid_search.fit(tfidf_matrix, train_df['time_consumed_millis'])\n",
    "\n",
    "best_params = grid_search.best_params_\n",
    "print(\"Best hyperparameters:\", best_params)\n",
    "\n",
    "best_model = grid_search.best_estimator_\n",
    "predicted_values = best_model.predict(test_tfidf_matrix)\n",
    "\n",
    "mse = mean_squared_error(test_df['time_consumed_millis'], predicted_values)\n",
    "rmse = np.sqrt(mse)\n",
    "print(\"Root Mean Squared Error (RMSE) of the best model:\", rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "318f38ce-190d-42eb-b324-079f8fa52d80",
   "metadata": {},
   "source": [
    "Best hyperparameters: {'activation': 'logistic', 'alpha': 0.5, 'hidden_layer_sizes': (439,), 'max_iter': 1500, 'solver': 'adam'}\n",
    "\n",
    "Tuned model : Root Mean Squared Error (RMSE) of the best model: 358.45408676412364"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b1ae43-bf5f-4e80-bb18-48240a920205",
   "metadata": {},
   "source": [
    "### Observations\n",
    "Current error is quite high. This is indicative that either our data either has errors or our model has low predictive abilities. I found in the data that we were conserving runtime errors which made it so we had skewed outputs for time in milliseconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "318236ce-35cb-4a28-b92a-3ec29bbb8f2f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
