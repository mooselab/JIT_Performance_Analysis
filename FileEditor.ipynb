{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e6db105-253c-4b41-b2e2-97fe008f31c7",
   "metadata": {},
   "source": [
    "### Finding the main method of a java file and adding @Benchmark annotation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c793dd34-b46c-4836-b489-b6f79715ed0b",
   "metadata": {},
   "source": [
    "This program will iterate over every file in the directory and add @Benchmark tag over each function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "204ff9aa-0cfb-44cc-8e86-20d35c3caa81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed annotation for file : ForwardBackward.java \n",
      "Completed annotation for file : Recursive.java \n",
      "Completed annotation for file : Shifting.java \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "# Define the folder containing Java files\n",
    "folder_path = r'C:\\Users\\Mettle\\Desktop\\Java-master\\java-combinations\\src\\main\\java\\com\\hmkcode'\n",
    "\n",
    "# Create the 'benchmark_files' folder if it doesn't exist\n",
    "if not os.path.exists('benchmark_files'):\n",
    "    os.makedirs('benchmark_files')\n",
    "\n",
    "# Function to process each Java file\n",
    "def process_java_file(java_file_path):\n",
    "    benchmark_file_path = os.path.join('benchmark_files', os.path.basename(java_file_path))\n",
    "    add_benchmark_annotation(java_file_path, benchmark_file_path)\n",
    "    add_counters(benchmark_file_path)\n",
    "    method_invocation_annotation(benchmark_file_path)\n",
    "   \n",
    "# Iterate over each file in the folder\n",
    "for file_name in os.listdir(folder_path):\n",
    "    if file_name.endswith('.java'):\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "        process_java_file(file_path)\n",
    "        print(f\"Completed annotation for file : {file_name} \")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cab5b9a-2d4b-49f9-b3ae-0c050d45e39c",
   "metadata": {},
   "source": [
    "#### Loop counter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c553d0ad-1976-4a90-be21-bb00cf18b5b0",
   "metadata": {},
   "source": [
    "*Currently not handling case with loops without braces.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d1fdf559-bb59-4e5a-8244-713c196ec384",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_counters(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "            java_code = file.read()\n",
    "    # Regex pattern for finding for loops and while loops\n",
    "    pattern = r'(?P<loop>(for\\s*\\([^\\)]*\\)\\s*\\{)|(while\\s*\\([^\\)]*\\)\\s*\\{))'\n",
    "    \n",
    "    def add_counter(match):\n",
    "        loop = match.group('loop')\n",
    "        return loop + '\\n    counter++;'\n",
    "    \n",
    "    # Replace each loop with the loop plus counter++\n",
    "    modified_java_code = re.sub(pattern, add_counter, java_code)\n",
    "    with open(file_path, 'w') as file:\n",
    "        file.write(modified_java_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8afbe892-3588-4713-b559-34fc72f244aa",
   "metadata": {},
   "source": [
    "#### @Benchmark Annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9a0d1066-9a92-4f1c-a1e2-03b2382451fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_benchmark_annotation(file_path, benchmark_file_path):\n",
    "    pattern = r'(?<!\\/\\/)(?<!\\/\\*)\\b(?:public\\s+|private\\s+|protected\\s+|static\\s+|final\\s+|native\\s+|synchronized\\s+|abstract\\s+|transient\\s+)*[\\$_\\w<>\\[\\]]*\\s+\\w+\\s*\\([^\\)]*\\)?\\s*\\{'\n",
    "\n",
    "    with open(file_path, 'r') as file:\n",
    "        java_code = file.read()\n",
    "\n",
    "    # Find all occurrences of method declarations in the Java code\n",
    "    matches = re.finditer(pattern, java_code)\n",
    "\n",
    "    modified_java_code = ''\n",
    "    previous_end_index = 0\n",
    "\n",
    "    # Loop through each method occurrence\n",
    "    for match in matches:\n",
    "        start_index = match.start()\n",
    "        end_index = match.end()\n",
    "        \n",
    "        method_declaration = java_code[start_index:end_index]\n",
    "        \n",
    "        # Insert @Benchmark annotation just above the method declaration\n",
    "        modified_java_code += java_code[previous_end_index:start_index] + '@Benchmark\\n' + method_declaration\n",
    "\n",
    "        previous_end_index = end_index\n",
    "\n",
    "    modified_java_code += java_code[previous_end_index:]\n",
    "    with open(benchmark_file_path, 'w') as file:\n",
    "        file.write(modified_java_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "302ebcf6-df6c-493e-90a3-55709e87819b",
   "metadata": {},
   "source": [
    "#### Count method invocation values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "73787349-7e31-4974-94c2-00320c8eae95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def method_invocation_annotation(file_path):\n",
    "    method_pattern = r'(?P<method>(?<!\\/\\/)(?<!\\/\\*)\\b(?:public\\s+|private\\s+|protected\\s+|static\\s+|final\\s+|native\\s+|synchronized\\s+|abstract\\s+|transient\\s+)*[\\$_\\w<>\\[\\]]*\\s+\\w+\\s*\\([^\\)]*\\)\\s*\\{[^\\}]*?\\})'\n",
    "    with open(file_path, 'r') as file:\n",
    "        java_code = file.read()\n",
    "    def add_counter(match):\n",
    "        method = match.group()\n",
    "        modified_method = method.replace('{', '{\\n    method_counter++;\\n')\n",
    "        return modified_method\n",
    "\n",
    "    # Replace each method with additional code added inside\n",
    "    modified_java_code = re.sub(method_pattern, add_counter, java_code)\n",
    "    with open(file_path, 'w') as file:\n",
    "        file.write(modified_java_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e131558-8d65-418e-aa71-8814bcd2ecbe",
   "metadata": {},
   "source": [
    "# Getting Codeforces submissions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7c79329-86bd-432a-8de9-60144297e090",
   "metadata": {},
   "source": [
    "### Generate URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d0bfd351-d379-4df9-a216-4e0d84b930c8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import hashlib\n",
    "import time\n",
    "import random\n",
    "from bs4 import BeautifulSoup\n",
    "def get_url(from_val):\n",
    "    key = \"baa5c566fa5bdeb92494876e5bcac06b6798d8fe\"\n",
    "    secret = \"0d686bfc8a3c654a569b93e3d1d12803d25264f1\"\n",
    "    count = 1000\n",
    "    current_time = int(time.time())\n",
    "    random_number = random.randint(100000, 999999)\n",
    "    method_fetched = \"contest.status\"\n",
    "    string_to_hash = f\"{random_number}/{method_fetched}?apiKey={key}&contestId=1928&count={count}&from={from_val}&time={current_time}#{secret}\"\n",
    "    sha512_hash = hashlib.sha512(string_to_hash.encode()).hexdigest()\n",
    "    req_url = f\"https://codeforces.com/api/{method_fetched}?apiKey={key}&contestId=1928&count={count}&from={from_val}&time={current_time}&apiSig={random_number}{sha512_hash}\"\n",
    "    return req_url"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd8cd6ee-50a2-4773-b9de-9bd2ab0afdb8",
   "metadata": {},
   "source": [
    "### Next step is to make a crawler to incrementally fetch data from Codeforces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8ce05d3b-bb1d-4f12-8a55-94bb39875841",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import os.path\n",
    "from os import path\n",
    "\n",
    "# Function to check if file exists and create it if it doesn't\n",
    "def create_csv_file(file_path):\n",
    "    if not path.exists(file_path):\n",
    "        with open(file_path, 'w', newline='') as file:\n",
    "            # Create a CSV writer object\n",
    "            csv_writer = csv.writer(file)\n",
    "            # Write header row\n",
    "            csv_writer.writerow(['id', 'contest_id', 'author','creation_time_seconds', 'relative_time_seconds', 'problem_name', 'problem_type', 'programming_language', 'verdict', 'test_set', 'passed_test_count', 'time_consumed_millis', 'memory_consumed_bytes'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "d7409cde-548f-4055-b8a5-55c483b2330d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_to_csv(data, file_path):\n",
    "    fieldnames = ['id', 'contest_id', 'author','creation_time_seconds', 'relative_time_seconds', 'problem_name', 'problem_type', 'programming_language', 'verdict', 'test_set', 'passed_test_count', 'time_consumed_millis', 'memory_consumed_bytes']\n",
    "    \n",
    "    file_exists = os.path.isfile(file_path)\n",
    "    \n",
    "    with open(file_path, 'a', newline='') as file:\n",
    "        writer = csv.DictWriter(file, fieldnames=fieldnames)\n",
    "        if not file_exists:\n",
    "            writer.writeheader()  # Write header only if file is new\n",
    "        writer.writerow(data)\n",
    "\n",
    "def make_request_and_append_to_csv(from_val, file_path):\n",
    "    req_url = get_url(from_val)\n",
    "    response = requests.get(req_url)\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        for item in data['result']:\n",
    "            csv_data = {\n",
    "                'id': item['id'],\n",
    "                'contest_id': item['contestId'],\n",
    "                'author': item['author']['members'][0]['handle'],\n",
    "                'creation_time_seconds': item['creationTimeSeconds'],\n",
    "                'relative_time_seconds': item['relativeTimeSeconds'],\n",
    "                'problem_name': item['problem']['name'],\n",
    "                'problem_type': item['problem']['type'],\n",
    "                'programming_language': item['programmingLanguage'],\n",
    "                'verdict': item['verdict'],\n",
    "                'test_set': item['testset'],\n",
    "                'passed_test_count': item['passedTestCount'],\n",
    "                'time_consumed_millis': item['timeConsumedMillis'],\n",
    "                'memory_consumed_bytes': item['memoryConsumedBytes']\n",
    "            }\n",
    "            append_to_csv(csv_data, file_path)\n",
    "    else:\n",
    "        print(\"Request failed with status code:\", response.status_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2232d031-4501-456e-bdb5-e0dd4dc97aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of iterations\n",
    "num_requests = 200\n",
    "# Initial value to start 1.\n",
    "from_val = 1\n",
    "\n",
    "csv_file = 'output.csv'\n",
    "\n",
    "create_csv_file(csv_file)\n",
    "\n",
    "for i in range(num_requests):\n",
    "    make_request_and_append_to_csv(from_val, 'output.csv')\n",
    "    from_val += 1000  \n",
    "    time.sleep(5)\n",
    "    print(f\"Completed: {i} / {num_requests} , Index value : {from_val}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c74c5d48-62bf-4960-8213-b27af0dbaedd",
   "metadata": {},
   "source": [
    "### We can now filter and find submissions of interest\n",
    "\n",
    "**We've fetched all problems from contestid : 1928**\n",
    "\n",
    "Each contest contains several problems. For this contest, there are the following problems : \n",
    "\n",
    "- **A : Rectangle Cutting** (20,553 users attempted)\n",
    "- **B : Equalize** (14,643 users attempted)\n",
    "- **C : Physical education lesson** (7066 users attempted)\n",
    "- **D : Lonely Mountain Dungeons** (3588 users attempted)\n",
    "- **E : Modular Sequence** (1418 users attempted)\n",
    "- **F : Digital Patterns** (193 users attempted)\n",
    "  \n",
    "Now I filter by the problem and look for individuals that made multiple submissions for a given problem. In order to be considered, they must have a correct submissions (no compilation errors or wrong answer) and be written in C++ language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2840c413-bf0a-40d5-8ae0-908cb7428bcc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Adjust CSV reading parameters\n",
    "df = pd.read_csv('output.csv')\n",
    "# Remove entries with testset value 'WRONG_ANSWER'\n",
    "df = df[df['verdict'] != 'WRONG_ANSWER']\n",
    "df = df[df['verdict'] != 'COMPILATION_ERROR']\n",
    "df = df[df['programming_language'].str.contains('C++')]\n",
    "df = df[df['problem_name'] == 'Equalize']\n",
    "\n",
    "#Remove authors that only submitted once.\n",
    "author_counts = df['author'].value_counts()\n",
    "multiple_submissions_authors = author_counts[author_counts > 1].index\n",
    "\n",
    "# Filter the DataFrame to only keep elements where the author appears more than once\n",
    "filtered_df = df[df['author'].isin(multiple_submissions_authors)]\n",
    "\n",
    "filtered_df.to_csv('filtered_output.csv', index=False)\n",
    "\n",
    "#Retrieve submissions ids\n",
    "id_values = filtered_df['id'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e978cec8-e455-4c4d-8453-79a78e1fce14",
   "metadata": {},
   "source": [
    "### Now we fetch the code for all these submissions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f27a51a-ed77-48db-9b80-c9ea7e2a4f4d",
   "metadata": {},
   "source": [
    "To fetch the submissions, we create a webscraper. The website's DDoS protection eventually kicks in so we implement several strategies : \n",
    "- Randomized access times [11, 20] seconds\n",
    "- Randomized user agent property to make it seem like multiple users are accessing from same IP.\n",
    "- Selenium web driver rather than requests. This is necessary to load Javascript and prevent detection, as requests does not load Javascript and therefore makes it easy to detect. \n",
    "- Rotating proxy addresses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c773bb-7e50-446b-ad8d-01d29fd6fccc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install selenium\n",
    "!pip install random_user_agent\n",
    "!pip install lxml\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "from random_user_agent.user_agent import UserAgent\n",
    "from random_user_agent.params import SoftwareName, OperatingSystem\n",
    "from selenium.webdriver.common.proxy import Proxy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f2663d5-b553-464e-a7fe-bc9083f2ff8f",
   "metadata": {},
   "source": [
    "This fetches a list of usable proxy addresses, updated every ten minutes. We can execute this function every ten minutes and fetch all the new addresses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3da97f7d-8b1a-475c-8b41-73dbeb6cd179",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def fetch_proxy_urls(driver):\n",
    "    driver.get('https://www.sslproxies.org/')\n",
    "    \n",
    "    # Extract the HTML content of the page\n",
    "    page_source = driver.page_source\n",
    "\n",
    "    # Parse the HTML content using BeautifulSoup\n",
    "    soup = BeautifulSoup(page_source, 'html.parser')\n",
    "\n",
    "    # Find the table containing proxy information\n",
    "    proxy_table = soup.find('table', class_='table')\n",
    "\n",
    "    # Extract IP addresses and ports from the table rows\n",
    "    proxy_server_urls = []\n",
    "    if proxy_table:\n",
    "        rows = proxy_table.find_all('tr')\n",
    "        for row in rows[1:]:  # Skip the header row\n",
    "            columns = row.find_all('td')\n",
    "            ip_address = columns[0].text\n",
    "            port = columns[1].text\n",
    "            proxy_url = f\"{ip_address}:{port}\"\n",
    "            proxy_server_urls.append(proxy_url)\n",
    "    driver.quit()\n",
    "    return proxy_server_urls\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a32a5c6-f2e2-4fc0-8a8e-c26ebfef4ddb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the directory to store code files\n",
    "directory = \"code_files\"\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "\n",
    "# Initialize UserAgent object\n",
    "software_names = [SoftwareName.EDGE.value, SoftwareName.CHROME.value, SoftwareName.CHROMIUM.value, SoftwareName.ANDROID.value, SoftwareName.FIREFOX.value, SoftwareName.OPERA.value, SoftwareName.SAFARI.value]\n",
    "operating_systems = [OperatingSystem.WINDOWS.value, OperatingSystem.LINUX.value, OperatingSystem.MAC.value]\n",
    "user_agent_rotator = UserAgent(software_names=software_names, operating_systems=operating_systems, limit=100)\n",
    "\n",
    "#chrome_options = Options()\n",
    "#chrome_options.add_argument(f'user-agent={user_agent}')\n",
    "#driver = webdriver.Chrome(options=chrome_options)\n",
    "#proxy_server_urls = fetch_proxy_urls(driver)\n",
    "count = 0\n",
    "# Iterate over each submission ID\n",
    "for submission_id in id_values:\n",
    "    try:\n",
    "        # Randomizing sleep duration (Must be greater than 10 to prevent DDOS protection from kicking in)\n",
    "        #if count % 35 == 0:\n",
    "        #    proxy_server_urls = fetch_proxy_urls(driver)\n",
    "     \n",
    "        # URL of the submission page\n",
    "        submission_url = f\"https://codeforces.com/contest/1928/submission/{submission_id}\"\n",
    "\n",
    "        # Set a random User-Agent for each request\n",
    "        user_agent = user_agent_rotator.get_random_user_agent()\n",
    "          \n",
    "        # Set up Chrome options\n",
    "        chrome_options = Options()\n",
    "\n",
    "        # Set proxy server URL\n",
    "        #PROXY = proxy_server_urls[random.randint(0, len(proxy_server_urls) - 1)]\n",
    "        #print(PROXY)\n",
    "        # Add user agent, incognito mode, and headless mode to Chrome options\n",
    "        chrome_options.add_argument(f'user-agent={user_agent}')\n",
    "        chrome_options.add_argument(\"--incognito\")\n",
    "        chrome_options.add_argument(\"--headless\")\n",
    "\n",
    "        # Set proxy server for Chrome WebDriver\n",
    "        #chrome_options.add_argument(\"--proxy-server=%s\" % PROXY)\n",
    "\n",
    "        # Initialize Chrome WebDriver with options\n",
    "        driver = webdriver.Chrome(options=chrome_options)\n",
    "\n",
    "        # Navigate to the submission page\n",
    "        driver.get(submission_url)\n",
    "        \n",
    "        # Check the response status code\n",
    "        response_code = driver.execute_script(\"return document.documentElement.outerHTML\").split('\\n')[0].split(' ')[1]\n",
    "        if response_code == '403':\n",
    "            print(f\"Received response code 403 for submission {submission_id}. Exiting loop.\")\n",
    "            break\n",
    "\n",
    "        # Extract the HTML content of the page\n",
    "        page_source = driver.page_source\n",
    "        \n",
    "        # Parse the HTML content using BeautifulSoup\n",
    "        soup = BeautifulSoup(page_source, 'lxml')\n",
    "\n",
    "        # Extract the code from the page\n",
    "        code_element = soup.find('pre', class_='prettyprint')\n",
    "        \n",
    "        if code_element:\n",
    "            code = code_element.get_text()\n",
    "\n",
    "            # Write the code to a .txt file\n",
    "            file_path = os.path.join(directory, f\"submission_{submission_id}.txt\")\n",
    "            with open(file_path, \"w\", encoding=\"utf-8\") as file:\n",
    "                file.write(code)\n",
    "            print(f\"Code found for submission {submission_id}.\")\n",
    "            count = 0\n",
    "        else:\n",
    "            print(f\"No code found for submission {submission_id}.\")\n",
    "            time.sleep(120)\n",
    "            count = count + 1\n",
    "            if count > 3:\n",
    "                driver.quit()\n",
    "                break\n",
    "        \n",
    "        time.sleep(random.randint(15, 20))\n",
    "        driver.quit()\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred for submission {submission_id}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45d09e5b-cec8-4eeb-b364-4749161b82aa",
   "metadata": {},
   "source": [
    "## Creating Bag of Words model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7620baa6-c403-4778-ba2f-b1085903552e",
   "metadata": {},
   "source": [
    "#### Pre-processing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "629ad3c7-f4a1-420e-b8fa-b01bb4184f5d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install nltk\n",
    "!pip install sctokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "215d8203-6fc9-4285-ae98-f29cc83e5e06",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from sctokenizer import CppTokenizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "model_csv\n",
    "\n",
    "def preprocess_code(code):\n",
    "    # Replace \";\" with newline character\n",
    "    code = code.replace(';', ';\\n')\n",
    "    return code\n",
    "\n",
    "def tokenize_cpp_file(file_path):\n",
    "    tokenizer = CppTokenizer()\n",
    "    with open(file_path) as file:\n",
    "        source = file.read()\n",
    "        # Tokenize preprocessed code\n",
    "        code = preprocess_code(source)\n",
    "        tokens = tokenizer.tokenize(code)\n",
    "        return ' '.join(token.token_value for token in tokens)  # Join tokens into a single string\n",
    "\n",
    "def process_code_files(directory):\n",
    "    corpus = []  # List to store tokenized code from all files\n",
    "    for file_name in os.listdir(directory):\n",
    "        if file_name.endswith(\".txt\"):  # Process only .txt files\n",
    "            file_path = os.path.join(directory, file_name)\n",
    "            tokens = tokenize_cpp_file(file_path)\n",
    "            corpus.append(tokens)\n",
    "\n",
    "    # Apply TF-IDF processing\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = vectorizer.fit_transform(corpus)\n",
    "\n",
    "    return vectorizer, tfidf_matrix\n",
    "\n",
    "directory = \"code_files\"\n",
    "vectorizer, tfidf_matrix = process_code_files(directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1aef5ca5-dd94-466b-8c41-56c41ee7b87c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sctokenizer import CppTokenizer\n",
    "\n",
    "def preprocess_code(code):\n",
    "    code = code.replace(';', ';\\n')\n",
    "    return code\n",
    "\n",
    "def tokenize_cpp_file(file_path):\n",
    "    tokenizer = CppTokenizer()\n",
    "    with open(file_path) as file:\n",
    "        source = file.read()\n",
    "        code = preprocess_code(source)\n",
    "        tokens = tokenizer.tokenize(code)\n",
    "        return ','.join(token.token_value for token in tokens)\n",
    "\n",
    "def process_code_files(directory, output_csv):\n",
    "    df_list = []\n",
    "\n",
    "    for file_name in os.listdir(directory):\n",
    "        if file_name.endswith(\".txt\"):\n",
    "            submission_id = file_name.split('_')[-1].split('.')[0]\n",
    "\n",
    "            file_path = os.path.join(directory, file_name)\n",
    "            tokens = tokenize_cpp_file(file_path)\n",
    "\n",
    "            df_list.append({'id': submission_id,\n",
    "                            'tokens': tokens})\n",
    "\n",
    "    df = pd.DataFrame(df_list)\n",
    "\n",
    "    output_df = pd.read_csv(output_csv)\n",
    "    output_df = output_df[['id', 'time_consumed_millis', 'memory_consumed_bytes']]\n",
    "\n",
    "  \n",
    "    df['id'] = df['id'].astype(str)\n",
    "    output_df['id'] = output_df['id'].astype(str)\n",
    "\n",
    "\n",
    "    df = pd.merge(df, output_df, on='id', how='left')\n",
    "\n",
    "    # Save final_df to CSV\n",
    "    final_df.to_csv('model_data.csv', index=False)\n",
    "\n",
    "directory = \"code_files\"\n",
    "output_csv = \"output.csv\"\n",
    "process_code_files(directory, output_csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34289ff5-4b17-4967-ba0c-5c9eb16e97af",
   "metadata": {},
   "source": [
    "## Constructing the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30a4a6cf-d696-426f-b61f-6468aaed7328",
   "metadata": {},
   "source": [
    "We're going to make 2 models : \n",
    "1. Execution time\n",
    "2. Memory consumption"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48d1e1f6-e0f1-4edd-9fe0-41b6fadfca9f",
   "metadata": {},
   "source": [
    "#### Splitting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2937adbc-e3cf-44e8-ae9d-4dc333baa9b9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df = pd.read_csv('model_data.csv')\n",
    " \n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f2dcca7-617e-40ad-8c5f-627c4fac4e17",
   "metadata": {},
   "source": [
    "#### Apply TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "72d584f5-3644-4e01-9467-e37c844bac69",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "corpus = []\n",
    "\n",
    "# Iterate over each row in the DataFrame\n",
    "for index, row in train_df.iterrows():\n",
    "    tokens = row['tokens']\n",
    "    # Join tokens into a single string and add to the corpus\n",
    "    corpus.append(' '.join(tokens.split()))\n",
    "\n",
    "# Create TF-IDF vectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit the vectorizer to the corpus and transform the corpus into TF-IDF matrix\n",
    "tfidf_matrix = vectorizer.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0452558e-3962-4fad-9ce9-ed8ff4f1ce63",
   "metadata": {},
   "source": [
    "Applying TF-IDF to test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "81360192-ad31-4d97-b123-3e8ce2902e20",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create TF-IDF matrix for the test data\n",
    "test_corpus = []\n",
    "\n",
    "# Iterate over each row in the test DataFrame\n",
    "for index, row in test_df.iterrows():\n",
    "    tokens = row['tokens']\n",
    "    # Join tokens into a single string and add to the test_corpus\n",
    "    test_corpus.append(' '.join(tokens.split()))\n",
    "\n",
    "# Transform the test_corpus into TF-IDF matrix using the same vectorizer\n",
    "test_tfidf_matrix = vectorizer.transform(test_corpus)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "521cf9f0-1dec-47b3-ba8d-6591c2d2454e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
